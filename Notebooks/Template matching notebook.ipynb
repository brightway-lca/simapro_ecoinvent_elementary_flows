{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d671a1da-94b0-4732-83d1-ffdc1801b110",
   "metadata": {},
   "source": [
    "# Template matching notebook\n",
    "\n",
    "This notebook provides some basic functionality to allow for flow matching and saving in the correct format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97dd6915-0af6-4563-8284-ad7b07d26036",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from datetime import datetime, timezone"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43643949-e038-412f-9b68-246389aa7ee0",
   "metadata": {},
   "source": [
    "Get paths of input and output directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aff657d-cb27-4dde-aea7-e3d907a6f55f",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data_dir = (Path.cwd().parent / \"Mapping\" / \"Input\" / \"Flowlists\").resolve()\n",
    "existing_matches_dir = (Path.cwd().parent / \"Mapping\" / \"Output\" / \"Mapped_files\").resolve()\n",
    "output_dir = (Path.cwd().parent / \"Contribute\").resolve()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99f057e2-4cc1-4ad1-8a97-eb6a523b0d1e",
   "metadata": {},
   "source": [
    "Read input dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df061563-9872-4d69-93de-b7aac700c31f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sp = pd.read_csv(input_data_dir / 'SimaProv9.4.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9b61570-d3dc-4549-9814-02002d94171a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ei = pd.read_csv(input_data_dir / 'ecoinventEFv3.7.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c9b47b3-253e-438d-be3a-0870258f6860",
   "metadata": {},
   "source": [
    "Example of how to combine dataframes using [merge](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.merge.html). We already have these matches, this is only an example :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4679c1b8-cd17-46c5-ad20-6b037f900d3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = sp.merge(ei, how=\"inner\", left_on=\"Flowable\", right_on=\"Flowable\")\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b0d3273-0ba6-4e44-bdd7-3cc52b1377ac",
   "metadata": {},
   "source": [
    "Adjust columns to match expected format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "031ccb41-da4e-4e5d-a678-7cca5faa8c43",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_names_after_merge(df):\n",
    "    mapping = {\n",
    "        'Flow UUID': 'SourceFlowUUID', \n",
    "        'FlowUUID': 'TargetFlowUUID',  # Incorrect column header in provided ecoinvent data\n",
    "        'Flowable_x': 'SourceFlowName', \n",
    "        'Flowable_y': 'TargetFlowName',\n",
    "        'Unit_x': 'SourceUnit',\n",
    "        'Unit_y': 'TargetUnit',\n",
    "        'Context_x': 'SourceFlowContext',\n",
    "        'Context_y': 'TargetFlowContext',\n",
    "    }\n",
    "    return df.rename(columns={k: v for k, v in mapping.items() if k in df.columns})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f417efd-2258-4608-9ccc-ad9d6cba69e4",
   "metadata": {},
   "source": [
    "Add some useful columns.\n",
    "\n",
    "* `author` is your name\n",
    "* `notebook_name` is the name of this notebook; we can't figure this out automatically. It should normally start with `Match -`.\n",
    "* `default_match_condition` is one of `=`, `~`, `<`, or `>`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53603a21-53d7-4c6c-94bd-73bd52cb96d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_common_columns(df, author, notebook_name, default_match_condition=\"=\"):\n",
    "    df['SourceListName'] = 'SimaPro9.4'\n",
    "    df['TargetListName'] = 'ecoinventEFv3.7'\n",
    "    df['MatchCondition'] = default_match_condition\n",
    "    df['Mapper'] = author\n",
    "    df['MemoMapper'] = f'Automated match. Notebook: {notebook_name}'\n",
    "    df['MemoSource'] = ''\n",
    "    df['MemoTarget'] = ''\n",
    "    df['MemoVerifier'] = ''\n",
    "    df['LastUpdated'] = datetime.now(timezone.utc).astimezone().isoformat()\n",
    "    df['Verifier'] = ''\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bdaa414-7582-4be3-966d-cb4cc5692107",
   "metadata": {},
   "source": [
    "Make sure the required columns are present"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d7a8f45-666a-4c86-82c2-e5e9a9c00d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_required_columns(df):\n",
    "    expected = set([     \n",
    "        \"SourceListName\", \"SourceFlowName\", \"SourceFlowUUID\", \"SourceFlowContext\", \"SourceUnit\", \n",
    "        \"MatchCondition\", \"TargetListName\", \"TargetFlowName\", \"TargetFlowUUID\", \n",
    "        \"TargetFlowContext\", \"TargetUnit\", \"Mapper\", \"Verifier\", \"LastUpdated\", \"MemoMapper\", \n",
    "        \"MemoVerifier\", \"MemoSource\", \"MemoTarget\"\n",
    "    ])\n",
    "    given = set(df.columns)\n",
    "    difference = expected.difference(given)\n",
    "    if difference:\n",
    "        print(\"Missing the following required columns:\", difference)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38805d39-e02e-4738-83ac-ea319a7914a3",
   "metadata": {},
   "source": [
    "Export the dataframe to the `contribute` directory. Please make your filename meaningful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bf16e12-6c54-4013-826e-cba7d7c1dc58",
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_dataframe(df, name):\n",
    "    SPEC_COLUMNS = [\n",
    "        \"SourceListName\", \"SourceFlowName\", \"SourceFlowUUID\", \"SourceFlowContext\", \"SourceUnit\", \n",
    "        \"MatchCondition\", \"ConversionFactor\", \"TargetListName\", \"TargetFlowName\", \"TargetFlowUUID\", \n",
    "        \"TargetFlowContext\", \"TargetUnit\", \"Mapper\", \"Verifier\", \"LastUpdated\", \"MemoMapper\", \n",
    "        \"MemoVerifier\", \"MemoSource\", \"MemoTarget\"\n",
    "    ]\n",
    "    \n",
    "    df = df[[col for col in SPEC_COLUMNS if col in df.columns]]\n",
    "    \n",
    "    if not name.lower().endswith(\".csv\"):\n",
    "        name += \".csv\"\n",
    "    \n",
    "    df.to_csv(output_dir / name, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
